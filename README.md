# CLIP Signals for Hazardous Scenes

> **Goal:** Detect and localise hazardous events in driving videos using CLIP similarity scores.
> *Original dataset:* 4 short clips (`video_0024â€“0031`)
> *Prompts per frame:* 5 (3 generic animal prompts + 2 videoâ€‘specific prompts)

---

## Repository Structure

```
clip-signals-for-hazardous-scenes/
â”œâ”€â”€ annotations/              # Ground-truth JSONs (scene_xxx.mp4.json)
â”œâ”€â”€ clip_scores/              # .npy similarity scores, prompt lists, eval CSVs
â”œâ”€â”€ dataset/                  # Raw videos (.mp4) â€“ ignored by git (email for access)
â”œâ”€â”€ graphs/                   # Similarity plots & threshold sweep results
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run/                  # Main pipeline scripts
â”‚   â”œâ”€â”€ debug/                # Dev-time utilities (e.g., video splitting)
â”‚   â””â”€â”€ setup/                # Setup and dependencies (e.g., CLIP wrappers)
â””â”€â”€ hpc_jobs/                 # SLURM job scripts for remote execution
```

---

## Pipeline Overview

1. **Frame Scoring** (`run_clip_similarity.py`)

   * For each video, compute CLIP similarity for **five prompts**:

     1. `animal`
     2. `animal crossing the road`
     3. `animal crossing the road unexpectedly`
     4. *video-specific prompt 1* (e.g. `dog crossing road`)
     5. *video-specific prompt 2* (e.g. `dog`)
   * Save results as:

     * `.npy` array of shape `(frames Ã— 5)`
     * associated `prompts.json`

2. **Evaluation** (`evaluate_clip_scores.py`)

   * Select the most relevant column:

     * If ground-truth hazard uses prompts 4â€“5, choose that column.
     * Else, use `max(axis=1)` over generic prompts (1â€“3).
   * Apply threshold `T = 0.25` to predict hazard frames.
   * Compare to ground-truth intervals via **Temporal IoU** and other metrics.

---

## What the Metrics Mean

| Metric            | Formula         | Intuition                                            |
| ----------------- | --------------- | ---------------------------------------------------- |
| **Precision (P)** | Â TPÂ /Â (TPÂ +Â FP) | "When I predict a hazard, how often am I right?"     |
| **Recall (R)**    | Â TPÂ /Â (TPÂ +Â FN) | "When a hazard exists, how often do I catch it?"     |
| **F1 Score**      | Â 2PRÂ /Â (PÂ +Â R)  | High only if both precision and recall are high      |
| **Temporal IoU**  | Â                | Overlap between predicted and ground-truth intervals |
| **Threshold (T)** | â€“               | CLIP similarity cutoff (default: 0.25)               |

**TP** = trueâ€‘positive frames Â Â Â Â **FP** = falseâ€‘positive Â Â Â Â **FN** = falseâ€‘negative

---

## Current Results (T = 0.25)

| Video    | TemporalÂ IoU | Precision | Recall    | F1Â Score  | Notes                                           |
| -------- | ------------ | --------- | --------- | --------- | ----------------------------------------------- |
| **0025** | **0.678**    | 0.757     | **0.867** | **0.808** | Best overall clip                               |
| 0030     | 0.630        | **0.970** | 0.643     | 0.773     | Very high precision; raise recall by lowering T |
| 0031     | 0.484        | 0.949     | 0.497     | 0.652     | Good precision, missed half GT frames           |
| 0024     | 0.444        | 0.829     | 0.488     | 0.615     | Similar toÂ 0031                                 |

ðŸ“„ CSV version: `clip_scores/clip_eval.csv`

---

## Next Steps

1. **Threshold Sweep**

   * Try `T âˆˆ [0.15 â€¦ 0.30]`, visualize IoU and F1

2. **Prompt Refinement**

   * Experiment with better hazard-specific phrasing

3. **False-Negative Inspection**

   * Visualize low-scoring but GT-labeled frames in videos 0024/0031

4. **Dataset Expansion**

   * Add more hazard categories (pedestrian, debris, construction)

---

## Quick Usage

```bash
# 1. Create environment
conda create -n clip_scores python=3.10 -y
conda activate clip_scores

# 2. Install dependencies
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121
pip install git+https://github.com/openai/CLIP.git opencv-python matplotlib tqdm numpy pillow

# 3. Run scoring pipeline
python scripts/run/run_clip_similarity.py

# 4. Run evaluation
python scripts/evaluate_clip_scores.py
```

---

ðŸ“© **Note:** For access to the full dataset (14+ driving clips), email `sshrir2@ucsc.edu`.
